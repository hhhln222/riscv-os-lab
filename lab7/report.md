# 实验7：文件系统

## 系统设计部分

### 架构设计说明

本实验实现了一个经典的分层文件系统，自底向上共分为四层：

1.  **设备驱动层 (`ramdisk.c`)**：
    *   为了规避复杂的 VirtIO 驱动配置，使用一块内存数组 (`uchar disk[]`) 模拟物理磁盘。
    *   实现了 `mkfs`，在内核启动时“格式化”磁盘，写入 Superblock 和根目录 Inode。

2.  **缓冲区缓存层 (`bio.c`)**：
    *   维护一个双向循环链表作为 LRU (Least Recently Used) 缓存池。
    *   提供 `bread` (读块) 和 `bwrite` (写块) 接口。
    *   确保同一磁盘块在内存中只有一份拷贝，保证数据一致性。

3.  **Inode 层 (`fs.c`)**：
    *   **元数据管理**：定义了磁盘 Inode (`dinode`) 和内存 Inode (`inode`)。
    *   **映射机制**：实现了 `bmap` 函数，将文件的逻辑偏移量映射到物理磁盘块号。
    *   **操作接口**：`readi` 和 `writei` 负责在内存缓冲区和 Inode 数据块之间搬运数据。

4.  **文件描述符层 (`file.c`, `sysfile.c`)**：
    *   抽象了 `struct file`，屏蔽了管道、设备和文件的区别。
    *   虽然本次实验主要验证内核级读写，但已为 `open`/`read`/`write` 系统调用做好了底层准备。

### 关键数据结构

**1. 磁盘布局**
```text
[ Boot | Superblock | Log | Inodes | Bitmaps | Data Blocks... ]
```
- **Superblock**: 描述文件系统总大小、Inode 区域位置等全局信息。
- **Inodes**: 固定大小的结构体数组，每个 Inode 描述一个文件（类型、大小、数据块指针）。

**2. 缓存块 (`struct buf`)**
```c
struct buf {
    int valid;   // 数据是否已读入
    int disk;    // 数据是否脏(需写回)
    uint dev;
    uint blockno;
    uint refcnt;
    struct buf *prev; // LRU 链表
    struct buf *next;
    uchar data[BSIZE]; // 1024字节数据
};
```

### 与xv6对比分析

| 模块 | xv6原版实现 | 本实验实现 | 差异说明 |
| :--- | :--- | :--- | :--- |
| **磁盘驱动** | `virtio_disk.c` (PCI/MMIO) | `ramdisk.c` (Memory Array) | 简化了硬件交互，专注于文件系统逻辑。xv6 使用异步中断驱动，本实验使用同步内存拷贝。 |
| **日志系统** | 完整的 Write-Ahead Log | 预留接口，未启用 | xv6 的日志层非常复杂，涉及事务提交。本实验侧重于文件读写路径，暂未开启崩溃恢复保护。 |
| **锁机制** | `sleeplock` | 无锁 / 简化锁 | 单核环境下，暂略去了复杂的锁竞争处理。 |

---

## 实验过程部分

### 实现步骤记录

1.  **定义数据结构**：根据 xv6 的设计，在 `fs.h` 中定义了磁盘布局，在 `file.h` 中定义了内存结构。
2.  **实现 RAM Disk**：编写 `ramdisk.c`，并实现简易的 `mkfs` 函数，在启动时初始化 Superblock 和根目录。
3.  **实现 Buffer Cache**：编写 `bio.c`，使用数组+链表管理 `struct buf`，实现 `bget`, `bread`, `brelse`。
4.  **实现核心 FS**：编写 `fs.c`，重点实现了 `iget` (获取Inode)、`ilock` (读取元数据) 和 `readi`/`writei` (读写数据)。
5.  **集成测试**：在 `test.c` 中直接调用内核 FS 函数，验证从 Inode 获取到数据读写的完整链路。

### 问题与解决方案

**问题1：类型定义报错 `unknown type name 'uint'`**
*   **现象**：编译时大量报错，提示找不到 `uint`、`ushort`。
*   **原因**：文件系统代码沿用了 UNIX 传统的简写类型，但基础 `types.h` 中未定义。
*   **解决**：在 `types.h` 中补全了 `typedef unsigned int uint;` 等定义。

**问题2：`bmap` 变量未使用警告**
*   **现象**：`error: unused variable 'bp'`。
*   **原因**：在简化版的 `bmap` 实现中，没有处理间接块，导致定义的变量没用到。
*   **解决**：删除了多余的变量声明。

**问题3：数据读写不一致 (`Data mismatch!`)**
*   **现象**：测试写入字符串后，读出来是空的。
*   **原因**：在 `writei` 函数中，数据写入了磁盘块，但忘记更新 `ip->size`。后续 `readi` 检查文件大小时发现是 0，直接返回。
*   **解决**：在 `writei` 末尾添加逻辑：`if (off + n > ip->size) ip->size = off + n;`。

### 源码理解总结

**Buffer Cache 的双重作用**
1.  **性能加速**：利用时间的局部性，最近访问的块留在内存中，避免慢速磁盘 I/O。
2.  **同步中心**：即便是 RAM Disk 这样极快的设备也需要 Cache。因为文件系统不同部分（如目录项和文件内容）可能修改同一个磁盘块。Cache 保证了内存中该块的唯一性，所有修改都在同一个 buffer 上进行，避免了数据不一致。

---

## 测试验证部分

### 功能测试结果

✅ **文件系统初始化**
- 成功识别魔数 `FSMAGIC`。
- 正确解析 Superblock，识别出文件系统大小为 1000 块。

✅ **元数据操作**
- `iget` 成功获取根目录 Inode (inum=1)。
- 验证根目录类型为 `T_DIR` (目录)，硬链接数为 1。

✅ **数据持久化**
- 向根目录写入 "Hello FS World!" (16字节)。
- 重新读取该位置，数据完全匹配。
- 这证明了 `writei` -> `bmap` -> `bread` -> `ramdisk` -> `bwrite` 的完整链路是通畅的。

### 运行截图
![alt text](image.png)

---

## 思考题

**1. 设计权衡：RAM Disk vs 真实磁盘**
在教学 OS 中，使用 RAM Disk 极大地降低了驱动调试难度，让学生能专注于文件系统的逻辑结构。但在真实 OS 中，这意味着数据在断电后会丢失。现代 OS 通常使用 Page Cache 结合具体的文件系统驱动（EXT4, NTFS）来管理物理磁盘。

**2. 性能优化：文件系统瓶颈**
当前实现的瓶颈在于：
1.  **线性搜索**：`iget` 遍历所有内存 inode 槽位。
2.  **全局锁**：`bio.c` 中的链表操作未加细粒度锁（虽然本实验是单核）。
3.  **同步写**：`bwrite` 是同步的，会阻塞 CPU。
优化方向包括使用哈希表管理 Inode 缓存、实现异步 I/O 和日志组提交。

**3. 可靠性：崩溃恢复**
当前系统如果中途崩溃，文件系统可能会损坏（例如：分配了 Inode 但未写入目录项）。xv6 通过 **Logging (日志)** 层解决这个问题：将多个写操作打包成一个原子事务，先写日志区，再写数据区。本实验简化了这一层，但在生产级系统中是必不可少的。
```